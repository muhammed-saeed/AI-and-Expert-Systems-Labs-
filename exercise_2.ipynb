{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammed-saeed/AI-and-Expert-Systems-Labs-/blob/main/exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-17T21:53:30.851389Z",
          "start_time": "2023-05-17T21:53:30.173211Z"
        },
        "id": "zkadQS0z69j5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "sys.path.append('..')\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw7tYcUp69j9"
      },
      "source": [
        "Deep neural networks have shown staggering performances in various learning tasks, including computer vision, natural language processing, and sound processing. They have made the model design more flexible by enabling end-to-end training.\n",
        "\n",
        "In this exercise, we get to have a first hands-on experience with neural network training. Many frameworks (e.g., PyTorch, Tensorflow, Caffe) allow easy usage of deep neural networks without precise knowledge of the inner workings of backpropagation and gradient descent algorithms. While these are very useful tools, it is important to get a good understanding of how to implement basic network training from scratch before using these libraries to speed up the process. For this purpose, we will implement a simple two-layer neural network and its training algorithm based on back-propagation using only basic matrix operations in questions 1 to 3. In question 4, we will use a popular deep learning library, PyTorch, to do the same and understand the advantages offered by using such tools.\n",
        "\n",
        "As a benchmark to test our models, we consider an image classification task using the widely used CIFAR-10 dataset. This dataset consists of 50000 training images of 32x32 resolution with 10 object classes, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The task is to code and train a parametrized model for classifying those images. This involves\n",
        "\n",
        "- Implementing the feedforward model (Question 1).\n",
        "- Implementing the backpropagation algorithm (gradient computation) (Question 2).\n",
        "- Training the model using stochastic gradient descent and improving the model training with better hyper-parameters (Question 3).\n",
        "- Using the PyTorch Library to implement the above and experiment with deeper networks (Question 4).\n",
        "- Refactoring the code into a modular Trainer class that takes care of everything (Question 5).\n",
        "\n",
        "A note on notation: Throughout the exercise, notation $v_i$ is used to denote the $i$-th element of vector $v$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4dx_Kmk69j_"
      },
      "source": [
        "### Question 1: Implementing the feedforward model (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZMJrQay69kA"
      },
      "source": [
        "In this question, we will implement a two-layered neural network architecture and the loss function to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3DPPSjb69kB"
      },
      "source": [
        "![](../../data/exercise-2/fig1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8BZDzGJ69kB"
      },
      "source": [
        "**Model architecture.** Our architecture is shown in Fig.1. It has an input layer and two model layers â€“ a hidden and an output layer. We start with randomly generated toy inputs of four dimensions and the number of classes K = 3 to build our model in Q1 and Q2, and in Q3 use images from the CIFAR-10 dataset to test our model on a real-world task. Hence input layer is 4-dimensional for now.\n",
        "\n",
        "In the hidden layer, there are 10 units. The input layer and the hidden\n",
        "layer are connected via linear weighting matrix $W^{(1)}\\in\\mathbb{R}^{10\\times\n",
        "4}$ and the bias term $b^{(1)}\\in\\mathbb{R}^{10}$. The parameters $W^{(1)}$\n",
        "and $b^{(1)}$ are to be learnt later on. A linear operation is performed,\n",
        "$W^{(1)}x+b^{(1)}$, resulting in a 10 dimensional vector $z^{(2)}$. It is then\n",
        "followed by a relu non-linear activation $\\phi$, applied element-wise on each\n",
        "unit, resulting in the activations $a^{(2)} = \\phi(z^{(2)})$. Relu function has\n",
        "the following form:\n",
        "\n",
        "\\begin{equation}\n",
        "\\phi(u) =  \\begin{cases}\n",
        "      u, & \\text{if}\\ u\\geq0 \\\\\n",
        "      0, & \\text{if}\\ u <0\n",
        "    \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "A similar linear operation is performed on $a^{(2)}$, resulting in $z^{(3)}=W^{(2)}a^{(2)}+b^{(2)}$, where $W^{(2)}\\in\\mathbb{R}^{3\\times 10}$ and $b^{(2)}\\in\\mathbb{R}^{3}$; it is followed by the softmax activation to result in $a^{(3)}=\\psi(z^{(3)})$. The softmax function is defined by:\n",
        "\\begin{equation}\n",
        "\\psi(u)_i =  \\frac{\\exp^{u_i}}{\\sum_j{\\exp^{u_j}}} \n",
        "\\end{equation}\n",
        "\n",
        "The final functional form of our model is thus defined by\n",
        "\n",
        "\\begin{align*}\n",
        "a^{(1)} &= x \\\\ \\\n",
        "z^{(2)} &= W^{(1)}a^{(1)}+b^{(1)} \\\\\n",
        "a^{(2)} &= \\phi(z^{(2)}) \\\\\n",
        "z^{(3)} &= W^{(2)}a^{(2)}+b^{(2)} \\\\\n",
        "f_\\theta(x) := a^{(3)} &= \\psi(z^{(3)}),\n",
        "\\end{align*}\n",
        "\n",
        "which takes a flattened 4 dimensional vector as input and outputs a $3$ dimensional vector, each entry in the output $f_k(x)$ representing the probability of image $x$ corresponding to the class $k$. We summarily indicate all the network parameters by $\\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDlU3YBp69kC"
      },
      "source": [
        "**Implementation.** We are now ready to implement the feedforward neural network.\n",
        "\n",
        "a) Implement the feedforward model. Verify that the scores you generate for the toy inputs match the correct scores. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-17T21:53:33.347045Z",
          "start_time": "2023-05-17T21:53:32.992095Z"
        },
        "id": "zKE0j3hz69kD"
      },
      "outputs": [],
      "source": [
        "import models.twolayernet.model as module_twolayernet\n",
        "from utils.utils import seed_everything, init_toy_data, rel_error\n",
        "from utils.gradient_check import eval_numerical_gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-17T21:53:34.132173Z",
          "start_time": "2023-05-17T21:53:34.106585Z"
        },
        "id": "OQKC-jPo69kD",
        "outputId": "6a9cbcdd-45a4-4bdd-83cb-7f8aa57344f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your scores:\n",
            "[[0.3644621  0.22911264 0.40642526]\n",
            " [0.47590629 0.17217039 0.35192332]\n",
            " [0.43035767 0.26164229 0.30800004]\n",
            " [0.41583127 0.2983228  0.28584593]\n",
            " [0.36328815 0.32279939 0.31391246]]\n",
            "\n",
            "Correct scores:\n",
            "[[0.3644621  0.22911264 0.40642526]\n",
            " [0.47590629 0.17217039 0.35192332]\n",
            " [0.43035767 0.26164229 0.30800004]\n",
            " [0.41583127 0.2983228  0.28584593]\n",
            " [0.36328815 0.32279939 0.31391246]]\n",
            "Difference between your scores and correct scores:\n",
            "2.9173411658645065e-08\n"
          ]
        }
      ],
      "source": [
        "seed_everything(1)\n",
        "\n",
        "input_size = 4\n",
        "hidden_size = 10\n",
        "num_classes = 3\n",
        "num_inputs = 5\n",
        "\n",
        "net = module_twolayernet.TwoLayerNetv1(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1)\n",
        "X, y = init_toy_data(num_inputs, input_size)\n",
        "\n",
        "scores = net.forward(X)\n",
        "print('Your scores:')\n",
        "print(scores)\n",
        "print()\n",
        "print('Correct scores:')\n",
        "correct_scores = np.asarray([\n",
        " [0.36446210, 0.22911264, 0.40642526],\n",
        " [0.47590629, 0.17217039, 0.35192332],\n",
        " [0.43035767, 0.26164229, 0.30800004],\n",
        " [0.41583127, 0.29832280, 0.28584593],\n",
        " [0.36328815, 0.32279939, 0.31391246]])\n",
        "print(correct_scores)\n",
        "\n",
        "# The difference should be very small. We get < 1e-7\n",
        "print('Difference between your scores and correct scores:')\n",
        "print(np.sum(np.abs(scores - correct_scores)))\n",
        "\n",
        "assert math.isclose(np.sum(np.abs(scores - correct_scores)), 0,  abs_tol=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsHtW4PI69kE"
      },
      "source": [
        "b) We later guide the neural network parameters\n",
        "$\\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$ to fit the given data and label\n",
        "pairs. We do so by minimising the loss function. A popular choice of the loss\n",
        "function for training neural networks for multi-class classification is the\n",
        "cross-entropy loss. For a single input sample $x_i$, with label $y_i$, the loss\n",
        "function is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "J(\\theta, x_i, y_i) &= -\\log{P(Y=y_i,X=x_i)} \\\\\n",
        "                    &= -\\log{f_\\theta(x_i)_{y_i}} \\\\\n",
        "                    &= -\\log{\\psi(z^{(3)})_{y_i}} \\\\\n",
        "J(\\theta, x_i, y_i) &= -\\log\\left[ \\frac{\\exp^{z^{(3)}_{y_i}}}{\\sum^K_j{\\exp^{z^{(3)}_j}}}\\right]\n",
        "\\end{align}\n",
        "\n",
        "Averaging over the whole training set, we get \n",
        "\n",
        "\\begin{align}\n",
        "J(\\theta,\\{x_i,y_i\\}_{i=1}^{N}) = \\frac{1}{N} \\sum_{i=1}^N \n",
        "-log\\left[ \\frac{\\exp^{z^{(3)}_{y_i}}}{\\sum_j{\\exp^{z^{(3)}_j}}}\\right],\n",
        "\\end{align}\n",
        "\n",
        "where $K$ is the number of classes. Note that if the model has perfectly fitted\n",
        "to the data (i.e. $f_\\theta^k(x_i)=1$ whenever $x_i$ belongs to class $k$ and 0\n",
        "otherwise), then $J$ attains the minimum of $0$. \n",
        "\n",
        "\n",
        "Apart from trying to correctly predict the label, we have to prevent\n",
        "overfitting the model to the current training data.  This is done by encoding\n",
        "our prior belief that the correct model should be simple (Occam's razor); we\n",
        "add an $L_2$ regularisation term over the model parameters $\\theta$.\n",
        "Specifically, the loss function is defined by:\n",
        "\n",
        "\\begin{align}\n",
        "\\tilde{J}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N\n",
        "-log\\left[ \\frac{\\exp^{z^{(3)}_{y_i}}}{\\sum_j{\\exp^{z^{(3)}_j}}}\\right]\n",
        "+\\lambda \\left(||W^{(1)}||_2^2 + ||W^{(2)}||_2^2 \\right),\n",
        "\\end{align}\n",
        "\n",
        "where $||\\cdot||_2^2$ is the squared $L_2$ norm. For example,\n",
        "\n",
        "\\begin{align}\n",
        "||W^{(1)}||_2^2 = \n",
        "\\sum_{p=1}^{10} \\sum_{q=1}^{4} W_{pq}^{(1)2}\n",
        "\\end{align}\n",
        "\n",
        "By changing the value of $\\lambda$ it is possible to give weights to your prior belief on the degree of simplicity (regularity) of the true model. \n",
        "\n",
        "Implement the final loss function and let it return the loss value. Verify the code by\n",
        "running and matching the output cost $1.30378789133$. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-17T21:53:44.890828Z",
          "start_time": "2023-05-17T21:53:44.875623Z"
        },
        "id": "kyiNDlRx69kF",
        "outputId": "46554a9f-0327-403d-8015-60a1476be8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Difference between your loss and correct loss:\n",
            "1.7985612998927536e-13\n"
          ]
        }
      ],
      "source": [
        "net_v2 = module_twolayernet.TwoLayerNetv2(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1) \n",
        "loss = net_v2.compute_loss(X, y, reg=0.05)\n",
        "correct_loss = 1.30378789133 # check this number with your implementation\n",
        "\n",
        "# should be very small, we get < 1e-12\n",
        "print('Difference between your loss and correct loss:')\n",
        "print(np.sum(np.abs(loss - correct_loss)))\n",
        "\n",
        "assert math.isclose(np.sum(np.abs(loss - correct_loss)), 0,  abs_tol=1e-6), 'The error with respect to the correct value is too high'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Djwl8uQ69kF"
      },
      "source": [
        "### Question 2: Backpropagation (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuftN-xm69kF"
      },
      "source": [
        "We train the model by solving\n",
        "\\begin{align}\n",
        "\\underset{\\theta}{\\min} \\,\\,  \\tilde{J}(\\theta)\n",
        "\\end{align}\n",
        "via stochastic gradient descent. Therefore, We need an efficient computation of the gradients $\\nabla_\\theta \\tilde{J}(\\theta)$. We use backpropagation of top layer error signals to the parameters $\\theta$ at different layers.\n",
        "\n",
        "In this question, you will be required to implement the backpropagation algorithm yourself from pseudocode. We will give a high-level description of what is happening in each line.\n",
        "\n",
        "For those who are interested in the robust derivation of the algorithm, we include the optional exercise on the derivation of the backpropagation algorithm. A piece of prior knowledge of standard vector calculus, including the chain rule, would be helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1nNr8BC69kG"
      },
      "source": [
        "**Backpropagation.** The backpropagation algorithm is simply a sequential application of the chain rule. It is applicable to any (sub-) differentiable model that is a composition of simple building blocks. In this exercise, we focus on the architecture with stacked layers of linear transformation + relu non-linear activation.\n",
        "\n",
        "The intuition behind the backpropagation algorithm is as follows. Given a training example $(x, y)$, we first run the feedforward to compute all the activations throughout the network, including the output value of the model $f_\\theta(x)$ and the loss $J$. Then, for each parameter in the model, we want to compute the effect that parameter has on the loss. This is done by computing the derivatives of the loss w.r.t for each model parameter.\n",
        "\n",
        "The backpropagation algorithm is performed from the top of the network (loss layer) to the bottom. It sequentially computes the gradient of the loss function with respect to each layer's activations and parameters.\n",
        "\n",
        "Letâ€™s start by deriving the gradients of the un-regularized loss function w.r.t final layer activations $z^{(3)}$. We will then use this in the chain rule to compute analytical expressions for gradients of all the model parameters.\n",
        "\n",
        "(a) Verify that the loss function (in Q1) has the gradient w.r.t $z^{(3)}$ as below.\n",
        "\\begin{equation}\n",
        "\\frac{\\partial J}{\\partial z^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) = \\frac{1}{N}\\left(\\psi(z^{(3)}) - \\Delta\\right), \n",
        "\\end{equation}\n",
        "where $\\Delta$ is a matrix of $N\\times K$ dimensions with \n",
        "\\begin{align}\n",
        "        \\Delta_{ij} = 1, & \\text{if}\\ y_i =j \\\\\n",
        "                0, & \\text{otherwise}\n",
        "\\end{align}\n",
        "(please write your answer in the block below, or attach an image in the same cell, 2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH6Vyq3n69kG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcrU4FIT69kG"
      },
      "source": [
        "b) To compute the effect of the weight matrix $W^{(2)}$ on the loss (in Q1) incurred by the network, we compute the\n",
        "partial derivatives of the loss function with respect to $W^{2}$.  This is done\n",
        "by applying the chain rule. Verify that the partial derivative of the loss w.r.t $W^{(2)}$ is  \n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial J}{\\partial W^{(2)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) &= \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
        "&= \\frac{1}{N} (\\psi(z^{(3)}) - \\Delta) a^{(2)'}\n",
        "\\end{align}\n",
        "\n",
        "Similarly, verify that the regularized loss has the derivatives\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial \\tilde{J}}{\\partial W^{(2)}} = \\frac{1}{N} (\\psi(z^{(3)}) - \\Delta) a^{(2)'} + 2\\lambda W^{(2)}\n",
        "\\end{align}\n",
        "\n",
        "(please write your answer in the block below, or attach an image in the same cell, 2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuSCh9I569kH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMajDzIf69kH"
      },
      "source": [
        "c) We can repeatedly apply chain rule as discussed above to obtain the derivatives of the loss with respect to all the parameters of the model $\\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$.\n",
        "Dervive the expressions for the derivatives of the regularized loss (in Q1) w.r.t $W^{(1)}$, $b^{(1)}$, $b^{(2)}$ now.\n",
        "(please write your answer in the block below, or attach an image in the same cell, 6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKiyYqtB69kH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyU2-dw_69kI"
      },
      "source": [
        "d) Using the expressions you obtained for the derivatives of the loss w.r.t model parameters, implement the back-propagation algorithm. Run the code and verify that the gradients you obtained are correct using numerical gradients (already\n",
        "implemented in the code). The maximum relative error between the gradients you compute and the numerical gradients should be less than 1e-8 for all parameters.\n",
        "(5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9aENnTV69kI"
      },
      "outputs": [],
      "source": [
        "net_v3 = module_twolayernet.TwoLayerNetv3(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1)\n",
        "loss, grads = net_v3.back_propagation(X, y, reg=0.05)\n",
        "\n",
        "# these should all be less than 1e-8 or so\n",
        "for param_name in grads:\n",
        "    f = lambda W: net_v3.back_propagation(X, y, reg=0.05)[0]\n",
        "    param_grad_num = eval_numerical_gradient(f, net_v3.params[param_name], verbose=False)\n",
        "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjAMB1Sj69kI"
      },
      "source": [
        "### Question 3: Stochastic gradient descent training (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVTab_er69kI"
      },
      "source": [
        "We have implemented the backpropagation algorithm for computing the parameter gradients and have verified that it indeed gives the correct gradient. We are now ready to train the network. We solve Eq.15 with the stochastic gradient descent.\n",
        "\n",
        "Typically neural networks are large and are trained with millions of data\n",
        "points. It is thus often infeasible to compute the gradient $\\nabla_\\theta\n",
        "\\tilde{J}(\\theta)$ that requires the accumulation of the gradient over the\n",
        "entire training set. Stochastic gradient descent addresses this problem by\n",
        "simply accumulating the gradient over a small random subset of the training\n",
        "samples (minibatch) at each iteration. Specifically, the algorithm is as\n",
        "follows,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8FCz5vC69kI"
      },
      "source": [
        "![](../../data/exercise-2/alg1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N17oRmmV69kJ"
      },
      "source": [
        "where the gradient $\\nabla_\\theta \\tilde{J}(\\theta,\\{(X^\\prime_j,y^\\prime_j)\\}_{j=1}^B)$ is computed only on the current randomly sampled batch.\n",
        "\n",
        "Intuitively, $v = -\\nabla_\\theta \\tilde{J}(\\theta^{(t-1)})$ gives the direction\n",
        "to which the loss $\\tilde{J}$ decreases the most (locally), and therefore we\n",
        "follow that direction by updating the parameters towards that direction\n",
        "$\\theta^{(t)} = \\theta^{(t-1)} + v$. \n",
        "\n",
        "a) Implement the stochastic gradient descent algorithm and run the training on the toy data. Your model\n",
        "should be able to  obtain loss <= 0.02 on the training set and the training\n",
        "curve should look similar to the one shown in figure 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aAYbz0869kJ"
      },
      "source": [
        "![](../../data/exercise-2/fig2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amPeOBHW69kJ"
      },
      "outputs": [],
      "source": [
        "net_v4 = module_twolayernet.TwoLayerNetv4(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1)\n",
        "stats = net_v4.train(X, y, X, y,\n",
        "                    learning_rate=1e-1, reg=5e-6,\n",
        "                    num_iters=100, verbose=False)\n",
        "\n",
        "print('Final training loss: ', stats['loss_history'][-1])\n",
        "\n",
        "# plot the loss history\n",
        "plt.plot(stats['loss_history'])\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('training loss')\n",
        "plt.title('Training Loss history')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e36IEu2I69kJ"
      },
      "source": [
        "b) We are now ready to train our model on a real image dataset. For this, we will use\n",
        "the CIFAR-10 dataset.  Since the images are of size $32\\times 32$ pixels with 3\n",
        "color channels, this gives us 3072 input layer units, represented by a vector\n",
        "$x\\in\\mathbb{R}^{3072}$. The code to load the data and train the model is provided with\n",
        "some default hyperparameters. With default\n",
        "hyperparameters, if previous questions have been done correctly, you should get\n",
        "a validation set accuracy of about 29\\%. This is very poor.\n",
        "Your task is to debug the model training and come up with better hyperparameters\n",
        "to improve the performance on the validation set.\n",
        "Visualize the training and validation performance curves to help with this analysis.\n",
        "There are several pointers provided in the comments to \n",
        "help you understand why the network might be underperforming.\n",
        "Once you have tuned your hyperparameters, and get validation accuracy greater\n",
        "than 48\\% run your best model on the test set once and report the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCRAhMIk69kJ"
      },
      "source": [
        "**Download CIFAR-10 using this link: <http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz>**\n",
        "<br/>\n",
        "**Decompress the downloaded dataset, and put the `cifar-10-batches-py` folder in the folder `data/exercise-2`**\n",
        "\n",
        "Please do not commit the dataset to git. This would make the repository unnecessarily large.\n",
        "\n",
        "You can also try the next two commented lines (simply uncomment them and run the cell) for getting the dataset. The commands are tested on Linux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0o8OC4m69kJ"
      },
      "outputs": [],
      "source": [
        "# !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "# !tar -xzf cifar-10-python.tar.gz -C ../../data/exercise-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNefoaqh69kK"
      },
      "outputs": [],
      "source": [
        "from utils.utils import show_net_weights\n",
        "from utils.data_utils import get_CIFAR10_data\n",
        "from utils.vis_utils import visualize_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fa_fygU69kK"
      },
      "outputs": [],
      "source": [
        "# # Load the data\n",
        "# Now that you have implemented a two-layer network that passes\n",
        "# gradient checks and works on toy data, it's time to load up our favorite\n",
        "# CIFAR-10 data so we can use it to train a classifier on a real dataset.\n",
        "# Invoke the get_CIFAR10_data function to get our data.\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n",
        "\n",
        "# Visualize some images to get a feel for the data\n",
        "plt.imshow(visualize_grid(X_train[:100, :].reshape(100, 32,32, 3), padding=3).astype('uint8'))\n",
        "plt.gca().axis('off')\n",
        "plt.show()\n",
        "\n",
        "# # Train a network\n",
        "# To train our network we will use SGD. In addition, we will\n",
        "# adjust the learning rate with an exponential learning rate schedule as\n",
        "# optimization proceeds; after each epoch, we will reduce the learning rate by\n",
        "# multiplying it by a decay rate.\n",
        "\n",
        "input_size = 32 * 32 * 3\n",
        "hidden_size = 50\n",
        "num_classes = 10\n",
        "net = module_twolayernet.TwoLayerNetv4(input_size=input_size, hidden_size=hidden_size, output_size=num_classes)\n",
        "# Train the network\n",
        "stats = net.train(X_train, y_train, X_val, y_val,\n",
        "            num_iters=1000, batch_size=200,\n",
        "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
        "            reg=0.25, verbose=True)\n",
        "\n",
        "# Predict on the validation set\n",
        "val_acc = (net.predict(X_val) == y_val).mean()\n",
        "print('Validation accuracy: ', val_acc)\n",
        "\n",
        "# # Debug the training\n",
        "# With the default parameters we provided above, you should get a validation\n",
        "# accuracy of about 0.29 on the validation set. This isn't very good.\n",
        "#\n",
        "# One strategy for getting insight into what's wrong is to plot the loss\n",
        "# function and the accuracies on the training and validation sets during\n",
        "# optimization.\n",
        "#\n",
        "# Another strategy is to visualize the weights that were learned in the first\n",
        "# layer of the network. In most neural networks trained on visual data, the\n",
        "# first layer weights typically show some visible structure when visualized.\n",
        "\n",
        "# Plot the loss function and train / validation accuracies\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(stats['loss_history'])\n",
        "plt.title('Loss history')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(stats['train_acc_history'], label='train')\n",
        "plt.plot(stats['val_acc_history'], label='val')\n",
        "plt.title('Classification accuracy history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Visualize the weights of the network\n",
        "\n",
        "show_net_weights(net)\n",
        "\n",
        "# # Tune your hyperparameters\n",
        "#\n",
        "# **What's wrong?**. Looking at the visualizations above, we see that the loss\n",
        "# is decreasing more or less linearly, which seems to suggest that the learning\n",
        "# rate may be too low. Moreover, there is no gap between the training and\n",
        "# validation accuracy, suggesting that the model we used has low capacity, and\n",
        "# that we should increase its size. On the other hand, with a very large model\n",
        "# we would expect to see more overfitting, which would manifest itself as a\n",
        "# very large gap between the training and validation accuracy.\n",
        "#\n",
        "# **Tuning**. Tuning the hyperparameters and developing intuition for how they\n",
        "# affect the final performance is a large part of using Neural Networks, so we\n",
        "# want you to get a lot of practice. Below, you should experiment with\n",
        "# different values of the various hyperparameters, including hidden layer size,\n",
        "# learning rate, numer of training epochs, and regularization strength. You\n",
        "# might also consider tuning the learning rate decay, but you should be able to\n",
        "# get good performance using the default value.\n",
        "#\n",
        "# **Approximate results**. You should aim to achieve a classification\n",
        "# accuracy of greater than 48% on the validation set. Our best network gets\n",
        "# over 52% on the validation set.\n",
        "#\n",
        "# **Experiment**: You goal in this exercise is to get as good of a result on\n",
        "# CIFAR-10 as you can (52% could serve as a reference), with a fully-connected\n",
        "# Neural Network. Feel free implement your own techniques (e.g. PCA to reduce\n",
        "# dimensionality, or adding dropout, or adding features to the solver, etc.).\n",
        "\n",
        "# **Explain your hyperparameter tuning process in the report.**\n",
        "\n",
        "best_net = net # store the best model into this\n",
        "\n",
        "#################################################################################\n",
        "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
        "# model in best_net.                                                            #\n",
        "#                                                                               #\n",
        "# To help debug your network, it may help to use visualizations similar to the  #\n",
        "# ones we used above; these visualizations will have significant qualitative    #\n",
        "# differences from the ones we saw above for the poorly tuned network.          #\n",
        "#                                                                               #\n",
        "# Tweaking hyperparameters- by hand can be fun, but you might find it useful to  #\n",
        "# write code to sweep through possible combinations of hyperparameters          #\n",
        "#################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# visualize the weights of the best network\n",
        "stats = best_net.train(X_train, y_train, X_val, y_val,\n",
        "            num_iters=20000, batch_size=200,\n",
        "            learning_rate=0.7 * 1e-3, learning_rate_decay=0.90,\n",
        "            reg=0.20, verbose=True)\n",
        "\n",
        "# Predict on the validation set\n",
        "val_acc = (best_net.predict(X_val) == y_val).mean()\n",
        "print('Validation accuracy: ', val_acc)\n",
        "show_net_weights(best_net)\n",
        "\n",
        "\n",
        "# # Run on the test set\n",
        "# When you are done experimenting, you should evaluate your final trained\n",
        "# network on the test set; you should get above 48%.\n",
        "\n",
        "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
        "print('Test accuracy: ', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NeFaMG69kK"
      },
      "source": [
        "### Question 4: Implement multi-layer perceptron using PyTorch library (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9c4f6tn69kL"
      },
      "source": [
        "So far, we have implemented a two-layer network by explicitly writing down the expressions for the forward and backward computations and training algorithms using simple matrix multiplication primitives from the NumPy library.\n",
        "\n",
        "However there are many libraries available designed make experimenting with neural networks faster by abstracting away the details into re-usable modules. One such popular open-source library is PyTorch (https://pytorch.org/). In this final question we will use the PyTorch library to implement the same two-layer network we did before and train it on the Cifar-10 dataset. However, extending a two-layer network to a three or four layered one is a matter of changing two-three lines of code using PyTorch. We will take advantage of this to experiment with deeper networks to improve the performance on the CIFAR-10 classification.\n",
        "\n",
        "To install the pytorch library follow the instruction in\n",
        "https://pytorch.org/get-started/locally/ . If you have access to a Graphics Processing\n",
        "Unit (GPU), you can install the gpu verison and run the exercise on GPU for faster run\n",
        "times. If not, you can install the cpu version (select cuda version None) and run on the\n",
        "cpu. Having gpu access is not necessary to complete the exercise.  There are good tutorials\n",
        "for getting started with pytorch on their website (https://pytorch.org/tutorials/).\n",
        "\n",
        "a) Complete the code to implement a multi-layer perceptron network in the class\n",
        "`MultiLayerPerceptron`. This includes instantiating the\n",
        "required layers from `torch.nn` and writing the code for forward pass. Initially you \n",
        "should write the code for the same two-layer network we have seen before.\n",
        "(3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMz7JMHE69kL",
        "outputId": "08f6af47-0c47-4eba-85f4-661a66ca46c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/exercise-2/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170498071/170498071 [00:02<00:00, 72153970.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/exercise-2/cifar-10-python.tar.gz to ../../data/exercise-2/\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 1e-3)\n",
        "        m.bias.data.fill_(0.)\n",
        "\n",
        "def update_lr(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "#--------------------------------\n",
        "# Device configuration\n",
        "#--------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: %s'%device)\n",
        "\n",
        "#--------------------------------\n",
        "# Hyper-parameters\n",
        "#--------------------------------\n",
        "input_size = 32 * 32 * 3\n",
        "hidden_size = [50]\n",
        "num_classes = 10\n",
        "num_epochs = 20\n",
        "batch_size = 200\n",
        "learning_rate = 1e-3\n",
        "learning_rate_decay = 0.95\n",
        "reg=0.001\n",
        "num_training= 49000\n",
        "num_validation =1000\n",
        "train = True\n",
        "drop_prob=0.1\n",
        "\n",
        "#-------------------------------------------------\n",
        "# Load the CIFAR-10 dataset\n",
        "#-------------------------------------------------\n",
        "norm_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                     ])\n",
        "cifar_dataset = torchvision.datasets.CIFAR10(root='../../data/exercise-2/',\n",
        "                                           train=True,\n",
        "                                           transform=norm_transform,\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/exercise-2/',\n",
        "                                          train=False,\n",
        "                                          transform=norm_transform\n",
        "                                          )\n",
        "#-------------------------------------------------\n",
        "# Prepare the training and validation splits\n",
        "#-------------------------------------------------\n",
        "mask = list(range(num_training))\n",
        "train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
        "mask = list(range(num_training, num_training + num_validation))\n",
        "val_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
        "\n",
        "#-------------------------------------------------\n",
        "# Data loader\n",
        "#-------------------------------------------------\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ETgn-NCF69kL"
      },
      "outputs": [],
      "source": [
        "#======================================================================================\n",
        "# Q4: Implementing multi-layer perceptron in PyTorch\n",
        "#======================================================================================\n",
        "# So far we have implemented a two-layer network using numpy by explicitly\n",
        "# writing down the forward computation and deriving and implementing the\n",
        "# equations for backward computation. This process can be tedious to extend to\n",
        "# large network architectures\n",
        "#\n",
        "# Popular deep-learining libraries like PyTorch and Tensorflow allow us to\n",
        "# quickly implement complicated neural network architectures. They provide\n",
        "# pre-defined layers which can be used as building blocks to define our\n",
        "# network. They also enable automatic-differentiation, which allows us to\n",
        "# define only the forward pass and let the libraries perform back-propagation\n",
        "# using automatic differentiation.\n",
        "#\n",
        "# In this question we will implement a multi-layer perceptron using the PyTorch\n",
        "# library.  Please complete the code for the MultiLayerPerceptron, training and\n",
        "# evaluating the model. Once you can train the two layer model, experiment with\n",
        "# adding more layers and\n",
        "#--------------------------------------------------------------------------------------\n",
        "\n",
        "#-------------------------------------------------\n",
        "# Fully connected neural network with one hidden layer\n",
        "#-------------------------------------------------\n",
        "class MultiLayerPerceptron(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, num_classes, drop_prob):\n",
        "        super(MultiLayerPerceptron, self).__init__()\n",
        "        #################################################################################\n",
        "        # TODO: Initialize the modules required to implement the mlp with given layer   #\n",
        "        # configuration. input_size --> hidden_layers[0] --> hidden_layers[1] .... -->  #\n",
        "        # hidden_layers[-1] --> num_classes                                             #\n",
        "        # Make use of linear and relu layers from the torch.nn module                   #\n",
        "        #################################################################################\n",
        "        layers = []\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        last_dim = input_size\n",
        "        for dim in hidden_layers:\n",
        "            layers.append(nn.Linear(last_dim, dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(p=drop_prob))\n",
        "            last_dim = dim\n",
        "            \n",
        "        layers.append(nn.Linear(last_dim, num_classes))  # Last layer\n",
        "        \n",
        "        \n",
        "  \n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #################################################################################\n",
        "        # TODO: Implement the forward pass computations                                 #\n",
        "        # Note that you do not need to use the softmax operation at the end.            #\n",
        "        # Softmax is only required for the loss computation and the criterion used below#\n",
        "        # nn.CrossEntropyLoss() already integrates the softmax and the log loss together#\n",
        "        #################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input\n",
        "        out = self.layers(x)\n",
        "        return out\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "model = MultiLayerPerceptron(input_size, hidden_size, num_classes, drop_prob).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LKkSedF69kM"
      },
      "source": [
        "b) Complete the code to train the network. Make use of the loss function `torch.nn.CrossEntropyLoss` to compute the loss and `loss.backward()` to compute the gradients. Once gradients are computed, `optimizer.step()` can be invoked to update the model. Your should be able to achieve similar performance ($>$ 48\\% accuracy on the validation set) as in Q3. Report the final validation accuracy you achieve with a two-layer network. (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxPHk0Sv69kM",
        "outputId": "16a6c855-1540-4be8-d911-44a33d08919d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy is: 35.844897959183676 %\n",
            "Validataion accuracy is: 42.7 %\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "model.apply(weights_init)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
        "\n",
        "# Train the model\n",
        "lr = learning_rate\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total = 0 \n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # if(i>2):\n",
        "        #     break\n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #################################################################################\n",
        "        # TODO: Implement the training code                                             #\n",
        "        # 1. Pass the images to the model                                               #\n",
        "        # 2. Compute the loss using the output and the labels.                          #\n",
        "        # 3. Compute gradients and update the model using the optimizer                 #\n",
        "        # Use examples in https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
        "        #################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()), end='\\r')\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print('Train accuracy is: {} %'.format(100 * correct / total))\n",
        "    # Code to update the lr\n",
        "    lr *= learning_rate_decay\n",
        "    update_lr(optimizer, lr)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            ####################################################\n",
        "            # TODO: Implement the evaluation code              #\n",
        "            # 1. Pass the images to the model                  #\n",
        "            # 2. Get the most confident predicted class        #\n",
        "            ####################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Validataion accuracy is: {} %'.format(100 * correct / total))\n",
        "\n",
        "##################################################################################\n",
        "# TODO: Now that you can train a simple two-layer MLP using above code, you can  #\n",
        "# easily experiment with adding more layers and different layer configurations   #\n",
        "# and let the pytorch library handle computing the gradients                     #\n",
        "#                                                                                #\n",
        "# Experiment with different number of layers (atleast from 2 to 5 layers) and    #\n",
        "# record the final validation accuracies Report your observations on how adding  #\n",
        "# more layers to the MLP affects its behavior. Try to improve the model          #\n",
        "# configuration using the validation performance as the guidance. You can        #\n",
        "# experiment with different activation layers available in torch.nn, adding      #\n",
        "# dropout layers, if you are interested. Use the best model on the validation    #\n",
        "# set, to evaluate the performance on the test set once and report it            #\n",
        "##################################################################################\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')\n",
        "\n",
        "# Run the test code once you have your by setting train flag to false\n",
        "# and loading the best model\n",
        "\n",
        "best_model = None # torch.load()\n",
        "best_model = torch.load('model.ckpt')\n",
        "model.load_state_dict(best_model)\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        ####################################################\n",
        "        # TODO: Implement the evaluation code              #\n",
        "        # 1. Pass the images to the model                  #\n",
        "        # 2. Get the most confident predicted class        #\n",
        "        ####################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        if total == 1000:\n",
        "            break\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "model.apply(weights_init)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
        "\n",
        "# Train the model\n",
        "lr = learning_rate\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total = 0 \n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()), end='\\r')\n",
        "\n",
        "    print('\\nTrain accuracy is: {} %'.format(100 * correct / total))\n",
        "\n",
        "    # Code to update the lr\n",
        "    lr *= learning_rate_decay\n",
        "    update_lr(optimizer, lr)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Validation accuracy is: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')\n",
        "\n",
        "# Load the best model\n",
        "best_model = torch.load('model.ckpt')\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if total == 1000:\n",
        "            break\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y66LurXz8ede",
        "outputId": "e081ee5f-9daa-47d0-84d8-fba11842ae20"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train accuracy is: 35.755102040816325 %\n",
            "Validation accuracy is: 40.9 %\n",
            "\n",
            "Train accuracy is: 42.755102040816325 %\n",
            "Validation accuracy is: 43.1 %\n",
            "\n",
            "Train accuracy is: 44.96530612244898 %\n",
            "Validation accuracy is: 46.2 %\n",
            "\n",
            "Train accuracy is: 46.39795918367347 %\n",
            "Validation accuracy is: 44.0 %\n",
            "\n",
            "Train accuracy is: 47.44489795918367 %\n",
            "Validation accuracy is: 45.4 %\n",
            "\n",
            "Train accuracy is: 48.138775510204084 %\n",
            "Validation accuracy is: 46.1 %\n",
            "\n",
            "Train accuracy is: 48.981632653061226 %\n",
            "Validation accuracy is: 44.3 %\n",
            "\n",
            "Train accuracy is: 49.70816326530612 %\n",
            "Validation accuracy is: 47.3 %\n",
            "\n",
            "Train accuracy is: 50.255102040816325 %\n",
            "Validation accuracy is: 46.5 %\n",
            "\n",
            "Train accuracy is: 50.577551020408166 %\n",
            "Validation accuracy is: 46.6 %\n",
            "\n",
            "Train accuracy is: 50.99183673469388 %\n",
            "Validation accuracy is: 46.5 %\n",
            "\n",
            "Train accuracy is: 51.66122448979592 %\n",
            "Validation accuracy is: 47.9 %\n",
            "\n",
            "Train accuracy is: 51.92857142857143 %\n",
            "Validation accuracy is: 48.0 %\n",
            "\n",
            "Train accuracy is: 51.910204081632656 %\n",
            "Validation accuracy is: 48.4 %\n",
            "\n",
            "Train accuracy is: 52.46530612244898 %\n",
            "Validation accuracy is: 48.4 %\n",
            "\n",
            "Train accuracy is: 52.72244897959184 %\n",
            "Validation accuracy is: 49.5 %\n",
            "\n",
            "Train accuracy is: 53.28775510204082 %\n",
            "Validation accuracy is: 48.3 %\n",
            "\n",
            "Train accuracy is: 53.42448979591837 %\n",
            "Validation accuracy is: 48.8 %\n",
            "\n",
            "Train accuracy is: 53.810204081632655 %\n",
            "Validation accuracy is: 50.3 %\n",
            "\n",
            "Train accuracy is: 54.179591836734694 %\n",
            "Validation accuracy is: 47.8 %\n",
            "Accuracy of the network on the 1000 test images: 48.9 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTpOwkt869kM"
      },
      "source": [
        "### Question 5: Modularized Implementation (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeXdbDWI69kV"
      },
      "source": [
        "In this part, we would like to familiarize ourselves with how often the training and evaluation is implemented using PyTorch. Technically, what we have already written above for previous questions already does what is needed. However, this might not be easily scalable when we want to explore different models, criterions, and hyper-parameters. We would need a proper config system to make changes easier and re-usable components so that we don't have to re-implement same thing multiple times.\n",
        "\n",
        "For this task, we ask you to basically plug-in the implementations from above cells (either what you've written or what was already given) into appropriate changes. Specifically, you need to implement the methods from `trainers/mlp_trainer.py`. Note that the class inherits from `BaseTrainer`, which also has many empty methods, **but you should only implement each one in the indicated class and not in the other**\n",
        "\n",
        "Specifically, you need to implement the indicated snipets in the following sections:\n",
        "\n",
        "1. In `trainers/mlp_trainer.py` in class `MLPTrainer`:\n",
        "\n",
        "`_train_epoch()`, `evaluate()`, `save_model()`, `load_model()`.\n",
        "\n",
        "2. In `trainers/base_trainer.py` in class `BaseTrainer`:\n",
        "\n",
        "`_do_evaluate()`\n",
        "\n",
        "\n",
        "3. In `models/mlp/metric.py` in class `TopKAccucacy`:\n",
        "\n",
        "`compute()`\n",
        "\n",
        "4. In `models/mlp/model.py` in class `MultiLayerPerceptron`:\n",
        "\n",
        "`__init__()`, `build_model()`, `forward()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKjL87Xu69kV"
      },
      "outputs": [],
      "source": [
        "from utils.parse_config import ConfigParser\n",
        "from trainers.mlp_trainer import MLPTrainer\n",
        "import data_loaders.data_loaders as module_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRo8s22O69kV"
      },
      "source": [
        "As you already know good hyperparams from the previous question, simply set those in the `../../cfgs/exercise-2/mlp_cifar10.json` file, to only train once with the right params!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkVrgxSJ69kV"
      },
      "outputs": [],
      "source": [
        "config = ConfigParser.wo_args('../../cfgs/exercise-2/mlp_cifar10.json')\n",
        "\n",
        "train_data_loader = config.init_obj('data_loader', module_data)\n",
        "valid_data_loader = train_data_loader.split_validation()\n",
        "\n",
        "trainer = MLPTrainer(config, train_loader=train_data_loader, eval_loader=valid_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dXGECzS69kW"
      },
      "outputs": [],
      "source": [
        "trainer.train() \n",
        "checkpoint_path = '../../saved/models/CIFAR10_MLP/latest.ckpt'\n",
        "trainer.save_model(path=checkpoint_path)\n",
        "# Note that the Trainer also saves some intermediate checkpoints itself\n",
        "# according to `save_period` in the config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV8rpYnQ69kW"
      },
      "outputs": [],
      "source": [
        "test_loader = getattr(module_data, config['data_loader']['type'])(\n",
        "                      config['data_loader']['args']['data_dir'],\n",
        "                      batch_size=512,\n",
        "                      shuffle=False,\n",
        "                      validation_split=0.0,\n",
        "                      training=False,\n",
        "                      num_workers=2\n",
        "                    )\n",
        "\n",
        "trainer.load_model(path='../../saved/models/CIFAR10_MLP/latest.ckpt') \n",
        "# Or replace the path with an epoch you already know has a good accuracy.\n",
        "\n",
        "result = trainer.evaluate(loader=test_loader)\n",
        "\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}